{"./":{"url":"./","title":"序言","keywords":"","body":"前言 本书记录本人学习kubernetes的经历，还正在路上，希望能带大家由浅入深、全面系统地学习kubernetes，节约大家的时间，少走弯路，从入门到精通。 kubernetes 很复杂，学习门槛较高，市面上的学习资料往往很难让小白快速入门，需要有一定的基础才能弄懂很多操作和概念。这本书正是为了让大多数人能快速入门甚至精通kubernetes而设计的，用接地气的方式在合适的阶段讲合适的东西，不会一下子讲的很全面，因为你也不可能一下子记住那么多东西，还会拖慢你的学习速度。总之，你只需要按照本书的思路走，就可以循序渐进的掌握kubernetes。 学习思路 kubernetes 本书是个非常复杂、非常灵活的东西，要真正会用它不能只是照搬网上搜索的操作步骤，一定要弄明白它的原理和概念，不然遇到问题就只能各种搜索，可能还解决不了。本书会按照下面的思路来讲： 简单介绍在安装 kubernetes 之前需要了解的基础概念和原理，为后面的安装铺路，让你看懂每一步在做什么和为什么这么做 教你如何安装 kubernetes （了解前面的基础马上装，不然学后面的概念和操作就纸上谈兵了） 把 kubernetes 涉及到的概念详细的列出来作为参考（这部分不需要一下子看完，后面用到的时候来翻下就行） 各种插件的安装与使用 Copyright © imroc.io 2018 all right reserved，powered by GitbookUpdated: 2018-03-09 15:07:27 "},"intro/introduce-kubernetes.html":{"url":"intro/introduce-kubernetes.html","title":"kubernetes简介","keywords":"","body":"Kubernetes 简介 什么是Kubernetes 简单来说就是一个容器的集群管理平台，你程序所需要的计算资源不再受单机资源的约束，可以将整个集群拥有的计算资源看成一块云，资源不够用就只需要增加机器，动态伸缩你的程序运行的数量就可以了。它不仅仅是集群管理，还可以让你只需要关注自己程序的逻辑，而服务发现、负载均衡、自动伸缩、滚动升级等这些你都不需要关心，它也不需要关心你用的什么语言，不侵入你的代码就能帮你完成这些，甚至它还有办法在你程序有bug的情况下尽量让你的程序正常提供服务。 kubernetes也是云原生的核心，整个生态飞速发展，日益壮大。服务网格(Service Mesh) 的出现如虎添翼，可以轻松实现和管理微服务，服务治理框架可能也将不再需要。 Kubernetes架构 为了更容易理解kubernetes架构，我先画了个简单版的架构图，屏蔽了很多细节 很容易看出来，kubernetes是用master来管理其它Node（工作负载的节点），当然master自身也是可以安装成Node的。 核心组件说明： apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制； etcd 保存了整个集群的状态； controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler 负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上； kubelet 负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理； kube-proxy 负责为Service提供cluster内部的服务发现和负载均衡； 如何操作集群 操作集群方式有多种，它们的原理都是调用 API Server 的接口。 通过 kubectl 命令行工具 给集群装 dashboard 插件，通过网页可视化操作 用 Rancher 来管理 kubernetes，功能更加丰富 利用 SDK 写程序来做一些集群操作自动化 不管怎样，最终都要和 API Server 通信，都需要经过访问控制校验，kubernetes 主要利用 RBAC (基于角色的访问控制）来做的，接下来会将。 Copyright © imroc.io 2018 all right reserved，powered by GitbookUpdated: 2018-03-09 15:07:27 "},"intro/kubernetes-authentication.html":{"url":"intro/kubernetes-authentication.html","title":"kubernetes权限控制","keywords":"","body":"Kubernetes 权限控制 \bKubernetes 的权限控制这块是\b理解其搭建\b过程中比较复杂也是比较重要的部分，这里先简单介绍下相关概念，为后面打下基础。 Namespace Kubernetes 集群中可包含多个 namespace，它们在逻辑上相互隔离，比如测试和生产如果在同一个 Kubernetes 集群上，可以用 namaspace 将它们隔离开，互不干扰。当然也可以通过一些方式跨 namespace 访问和操作，前提是分配了足够的权限。 RBAC——基于角色的访问控制 Kubernetes 的权限控制主要使用基于角色的访问控制（Role-Based Access Control, 即”RBAC”）\b，简单来说，就是不管是\b集群管理员还是集群中的程序，把它们都看用户，它们要想对集群进行访问或操作，就需要相应的权限，权限通过角色\b来代表，每个角色可以被赋予一组权限，角色可以绑定到\b用户上，绑定之后用户就拥有了相应的权限。 用户与用户组 Kubernetes 集群中包含两类用户： \bUser : \b限制集群管理员的权限。比如刚开始学习我们可以都用最高管理员权限，可以在集群中任何 namespace 下进行访问和各种操作。到了生产环境，如果集群比较大，操作的人比较多，管理员权限的分配可能就需要更加细化了。 \bService Account : 服务账号，限制集群\b中运行的程序的权限。比如 Kubernetes 自身的组件或一些插件，往往它们都需要对整个集群的一些状态和数据进行读写操作，就需要相应的权限；而一些普通的程序可能不需要很高的权限，我们最好就不需要给那么高的权限，以免发生意外。 我们一般要创建的是 Service Account， 定义示例： apiVersion: v1 kind: ServiceAccount metadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile 用户组： Group : 用于给一组\b用户赋予相同的权限。 角色 角色用来代表一组权限，在 Kubernetes 中有两类角色： Role : 代表某个 namaspace 下的一组权限。 一个Role对象只能用于授予对某一单一命名空间中资源的访问权限。 以下示例描述了”default”命名空间中的一个Role对象的定义，用于授予对pod的读访问权限： kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: pod-reader rules: - apiGroups: [\"\"] # 空字符串\"\"表明使用core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] ClusterRole : 代表整个集群范围内的一组权限。 ClusterRole 定义示例： kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: # 鉴于ClusterRole是集群范围对象，所以这里不需要定义\"namespace\"字段 name: secret-reader rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] 角色绑定 可以给某个用户或某个用户组分配一组权限，通过角色绑定来实现。分两类： RoleBinding : 绑定的权限只作用于某个 namespace 下。 定义示例： # 以下角色绑定定义将允许用户\"jane\"从\"default\"命名空间中读取pod。 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: default subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io ClusterRoleBinding : 绑定的权限作用于整个集群。 定义示例：\b # 以下`ClusterRoleBinding`对象允许在用户组\"manager\"中的任何用户都可以读取集群中任何命名空间中的secret。 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: read-secrets-global subjects: - kind: Group name: manager apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 注： RoleBinding 中的 roleRef 也可以用 ClusterRole，只不过将 ClusterRole 中定义的权限限定在某 namespace 下，通常用于预先定义一些通用的角色，在多个 namespace 下复用。定义示例： # 以下角色绑定允许用户\"dave\"读取\"development\"命名空间中的secret。 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-secrets namespace: development # 这里表明仅授权读取\"development\"命名空间中的资源。 subjects: - kind: User name: dave apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io TLS Kubernetes 的权限校验是通过校验证书来实现的，提取证书中的 CN(Common Name) 字段作为用户名，O(Organization) 字段作为用户组\b。 Copyright © imroc.io 2018 all right reserved，powered by GitbookUpdated: 2018-03-09 15:07:27 "},"install-kubernetes/install-kubernetes.html":{"url":"install-kubernetes/install-kubernetes.html","title":"概述","keywords":"","body":"安装说明 kubernetes 本身非常灵活，把不同功能模块拆成了多个独立运行的组件，有些组件还有第三方可替代品，还有各种插件，根据自己需求安装所需模块。 要找到适合自己需求的安装方式需要对 kubernetes 有着一定程度的理解，在前期学习的时候建议先搭建一个通用的简单版 kubernetes，这样能解决大部分普通需求，至少可以上手各种实践操作，后面再慢慢深入原理与概念。 Copyright © imroc.io 2018 all right reserved，powered by GitbookUpdated: 2018-03-09 15:07:27 "},"install-kubernetes/install-kubernetes-1.9-on-centos7-with-kubeadm.html":{"url":"install-kubernetes/install-kubernetes-1.9-on-centos7-with-kubeadm.html","title":"利用kubeadm在CentOS 7上部署Kubernetes v1.9","keywords":"","body":"准备 修改系统配置 开启路由转发（保证 proxy 正常运行，Service 需要） sysctl -w net.ipv4.ip_forward=1 默认情况下，由于安全原因，linux是关闭了路由转发的，即同台机器不止一个网卡，将数据包从一个网卡传到另一个网卡，让另一个网卡继续路由，即实现两个不同网段的主机通信。service 的 IP 是通过 proxy（即 kube-proxy 或 kube-router ）路由的，并不需要路由器参与，node 收到数据包时，数据包的目的 IP 为本机的内网 IP，proxy 将数据包的目的IP转化成Service IP并路由转发到Serive IP 对应网段的虚拟网卡上，最终路由到正确的Pod ip_forward 与路由转发：http://blog.51cto.com/13683137989/1880744 禁用 swap (保证 kubelet 正确运行): swapoff -a 关闭SELinux和防火墙 sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config # 修改配置永久生效，需重启 setenforce 0 #关闭防火墙 systemctl stop firewalld && systemctl disable firewalld RHEL / CentOS 7上的某些用户报告了由于iptables被绕过而导致流量被错误路由的问题。应该确保net.bridge.bridge-nf-call-iptables的sysctl配置中被设置为1 cat /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system 升级内核 运行docker的node节点需要升级到4.x内核支持overlay2驱动 检查内核 uname -sr 添加升级内核的第三方库 www.elrepo.org 上有方法 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 列出内核相关包 yum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available 安装最新稳定版 yum --enablerepo=elrepo-kernel install kernel-ml -y 查看内核默认启动顺序 awk -F\\' '$1==\"menuentry \" {print $2}' /etc/grub2.cfg 结果显示 CentOS Linux (4.15.7-1.el7.elrepo.x86_64) 7 (Core) CentOS Linux (3.10.0-693.17.1.el7.x86_64) 7 (Core) CentOS Linux (3.10.0-693.2.2.el7.x86_64) 7 (Core) CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core) CentOS Linux (0-rescue-f0f31005fb5a436d88e3c6cbf54e25aa) 7 (Core) 设置默认启动的内核，顺序index 分别是 0,1,2,3，每个人机器不一样，看清楚选择自己的index， 执行以下代码选择内核 grub2-set-default 0 重启 reboot 检查内核 uname -a docker 安装与配置 不建议使用官网的docker-ce版本、支持性不是很好、使用epel源支持的docker即可。 确保epel源已安装 yum install -y epel-release 安装docker yum install -y docker 使用overlay2驱动 docker 存储驱动很多默认用devicemapper，存在很多问题，最好使用overlay2，内核版本小于 3.10.0-693 的不要使用 overlay2 驱动。 确保 yum-plugin-ovl 安装，解决 ovlerlay2 兼容性问题： yum install -y yum-plugin-ovl overlay2 兼容性问题详见：https://docs.docker.com/storage/storagedriver/overlayfs-driver/#limitations-on-overlayfs-compatibility ： 备份 docker 用到的目录（若需要） cp -au /var/lib/docker /var/lib/docker.bk 关闭 docker systemctl stop docker 配置 docker 的存储驱动 vi /etc/docker/daemon.json { \"storage-driver\": \"overlay2\" } 如果使用 Docker EE 并且版本大于 17.06，还需要一个 storage-opts，这样配置 { \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } docker 设置 overlay2 驱动官方参考文档：https://docs.docker.com/storage/storagedriver/overlayfs-driver/#configure-docker-with-the-overlay-or-overlay2-storage-driver 启动 docker systemctl start docker 安装 kubeadm, kubectl, kubelet 配置国内kubernetes源 cat > /etc/yum.repos.d/kubernetes.repo 如果直接安装最新版可以直接这样做 yum install -y kubelet kubeadm kubectl 如果要指定版本，可以先看看有那些版本 [root@roc ~]# yum list kubeadm --showduplicates Loaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile * elrepo: mirrors.tuna.tsinghua.edu.cn Installed Packages kubeadm.x86_64 1.9.3-0 @kubernetes Available Packages kubeadm.x86_64 1.6.0-0 kubernetes kubeadm.x86_64 1.6.1-0 kubernetes kubeadm.x86_64 1.6.2-0 kubernetes kubeadm.x86_64 1.6.3-0 kubernetes kubeadm.x86_64 1.6.4-0 kubernetes kubeadm.x86_64 1.6.5-0 kubernetes kubeadm.x86_64 1.6.6-0 kubernetes kubeadm.x86_64 1.6.7-0 kubernetes kubeadm.x86_64 1.6.8-0 kubernetes kubeadm.x86_64 1.6.9-0 kubernetes kubeadm.x86_64 1.6.10-0 kubernetes kubeadm.x86_64 1.6.11-0 kubernetes kubeadm.x86_64 1.6.12-0 kubernetes kubeadm.x86_64 1.6.13-0 kubernetes kubeadm.x86_64 1.7.0-0 kubernetes kubeadm.x86_64 1.7.1-0 kubernetes kubeadm.x86_64 1.7.2-0 kubernetes kubeadm.x86_64 1.7.3-1 kubernetes kubeadm.x86_64 1.7.4-0 kubernetes kubeadm.x86_64 1.7.5-0 kubernetes kubeadm.x86_64 1.7.6-1 kubernetes kubeadm.x86_64 1.7.7-1 kubernetes kubeadm.x86_64 1.7.8-1 kubernetes kubeadm.x86_64 1.7.9-0 kubernetes kubeadm.x86_64 1.7.10-0 kubernetes kubeadm.x86_64 1.7.11-0 kubernetes kubeadm.x86_64 1.8.0-0 kubernetes kubeadm.x86_64 1.8.0-1 kubernetes kubeadm.x86_64 1.8.1-0 kubernetes kubeadm.x86_64 1.8.2-0 kubernetes kubeadm.x86_64 1.8.3-0 kubernetes kubeadm.x86_64 1.8.4-0 kubernetes kubeadm.x86_64 1.8.5-0 kubernetes kubeadm.x86_64 1.8.6-0 kubernetes kubeadm.x86_64 1.8.7-0 kubernetes kubeadm.x86_64 1.8.8-0 kubernetes kubeadm.x86_64 1.9.0-0 kubernetes kubeadm.x86_64 1.9.1-0 kubernetes kubeadm.x86_64 1.9.2-0 kubernetes kubeadm.x86_64 1.9.3-0 kubernetes 如果安装 1.9.3 ，执行下面的命令 yum install -y kubelet-1.9.3-0 kubeadm-1.9.3-0 kubectl-1.9.3-0 kubelet设置开机自动运行 systemctl enable kubelet kubelet启动参数增加 --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice 防止kubelet报错 vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 将 KUBELET_CGROUP_ARGS 一行改为： Environment=\"KUBELET_CGROUP_ARGS=--cgroup-driver=systemd --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice\" 然后reload并启动kubelet systemctl daemon-reload systemctl start kubelet 初始化master 国内快速下载镜像（假设docker配了加速器） docker pull docker.io/k8smirror/flannel:v0.9.1-amd64 docker tag docker.io/k8smirror/flannel:v0.9.1-amd64 quay.io/coreos/flannel:v0.9.1-amd64 docker rmi docker.io/k8smirror/flannel:v0.9.1-amd64 docker pull docker.io/gcrio/pause-amd64:3.0 docker tag docker.io/gcrio/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0 docker rmi docker.io/gcrio/pause-amd64:3.0 docker pull docker.io/gcrio/hyperkube:v1.9.3 docker pull docker.io/gcrio/etcd:3.1.11 创建 kubeadm 配置文件 cat kubeadm.yaml apiVersion: kubeadm.k8s.io/v1alpha1 kind: MasterConfiguration api: advertiseAddress: 0.0.0.0 unifiedControlPlaneImage: docker.io/gcrio/hyperkube:v1.9.3 selfHosted: true kubernetesVersion: v1.9.3 authorizationModes: - RBAC - Node kubeProxy: config: mode: ipvs # k8s 1.9开始kube-proxy的ipvs进入beta，替代iptables方式路由service etcd: image: docker.io/gcrio/etcd:3.1.11 # k8s 1.9 官方推荐的etcd版本为 3.1.11 imageRepository: gcrio featureGates: CoreDNS: true networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 EOF 执行初始化 [root@roc k8s]# kubeadm init --config kubeadm.yaml [init] Using Kubernetes version: v1.9.3 [init] Using Authorization modes: [RBAC Node] [preflight] Running pre-flight checks. [WARNING Hostname]: hostname \"roc\" could not be reached [WARNING Hostname]: hostname \"roc\" lookup roc on 100.100.2.138:53: no such host [WARNING FileExisting-crictl]: crictl not found in system path [preflight] Starting the kubelet service [certificates] Generated ca certificate and key. [certificates] Generated apiserver certificate and key. [certificates] apiserver serving cert is signed for DNS names [roc kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.17.60.67] [certificates] Generated apiserver-kubelet-client certificate and key. [certificates] Generated sa key and public key. [certificates] Generated front-proxy-ca certificate and key. [certificates] Generated front-proxy-client certificate and key. [certificates] Valid certificates and keys now exist in \"/etc/kubernetes/pki\" [kubeconfig] Wrote KubeConfig file to disk: \"admin.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"kubelet.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"controller-manager.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"scheduler.conf\" [controlplane] Wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" [controlplane] Wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" [controlplane] Wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" [etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\" [init] Waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\". [init] This might take a minute or longer if the control plane images have to be pulled. [apiclient] All control plane components are healthy after 29.501817 seconds [uploadconfig] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [markmaster] Will mark node roc as master by adding a label and a taint [markmaster] Master roc tainted and labelled with key/value: node-role.kubernetes.io/master=\"\" [bootstraptoken] Using token: 0e78a0.38a53399a9489d52 [bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join --token 0e78a0.38a53399a9489d52 172.17.60.67:6443 --discovery-token-ca-cert-hash sha256:038654a3d0adb79978913e5d2bce191b5d8536feac7d9354ca35b348e9fc4cd5 如果初始化失败，可以撤销 kubeadm reset 如果成功，根据输出提示，master 上如果想通过 kubectl 管理集群，执行下面的命令 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 最重要的是最后一行，保存下来，其它 node 加入集群需要执行那条命令 kubeadm join --token 0e78a0.38a53399a9489d52 172.17.60.67:6443 --discovery-token-ca-cert-hash sha256:038654a3d0adb79978913e5d2bce191b5d8536feac7d9354ca35b348e9fc4cd5 这个时候，如果你看 kubelet 的日志会发现不断提示没有cni插件，kubeadm 配置文件中的 CoreDNS 也不会生效 journalctl -xef -u kubelet -n 20 安装 flannel 网络插件就可以搞定了 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml 看看集群状态 [root@roc ~]# kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} 使用kubeadm初始化的集群，出于安全考虑Pod不会被调度到Master Node上，可使用如下命令使Master节点参与工作负载 kubectl taint nodes --all node-role.kubernetes.io/master- 输出类似下面（报错可忽略） node \"roc\" untainted error: taint \"node-role.kubernetes.io/master:\" not found worker 节点加入 下载需要的镜像 docker pull docker.io/gcrio/hyperkube:v1.9.3 docker pull docker.io/gcrio/pause-amd64:3.0 docker tag docker.io/gcrio/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0 docker rmi docker.io/gcrio/pause-amd64:3.0 将之前保存的 kubeadm join 命令粘贴过来 kubeadm join --token 0e78a0.38a53399a9489d52 172.17.60.67:6443 --discovery-token-ca-cert-hash sha256:038654a3d0adb79978913e5d2bce191b5d8536feac7d9354ca35b348e9fc4cd5 在master上看看集群节点 kubectl get nodes Copyright © imroc.io 2018 all right reserved，powered by GitbookUpdated: 2018-03-09 15:07:27 "},"install-kubernetes/install-kubernetes-1.9-on-centos7/install-kubernetes-1.9-on-centos7.html":{"url":"install-kubernetes/install-kubernetes-1.9-on-centos7/install-kubernetes-1.9-on-centos7.html","title":"在CentOS 7上部署Kubernetes v1.9","keywords":"","body":"在CentOS 7 上安装Kubernetes v1.9 Copyright © imroc.io 2018 all right reserved，powered by GitbookUpdated: 2018-03-09 15:07:27 "},"install-kubernetes/install-kubernetes-1.9-on-centos7/create-key-and-crt.html":{"url":"install-kubernetes/install-kubernetes-1.9-on-centos7/create-key-and-crt.html","title":"创建证书和密钥","keywords":"","body":"创建证书和密钥 安装 CFSSL wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x cfssl_linux-amd64 mv cfssl_linux-amd64 /usr/local/bin/cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x cfssljson_linux-amd64 mv cfssljson_linux-amd64 /usr/local/bin/cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 chmod +x cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo 创建 CA 创建 CA 配置文件 创建 ca-config.json 文件，它是后面每次生成证书的时候要用到的配置文件，内容如下： { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } 创建 CA 证书签名请求 创建 ca-csr.json 文件，它是生成CA证书和密钥用到的特有配置文件，内容如下： { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 生成 CA 证书和私钥 $ cfssl gencert -initca ca-csr.json | cfssljson -bare ca $ ls ca* ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem 创建Master证书 创建 kubernetes-csr.json 文件，内容如下： { \"CN\": \"kubernetes\", \"hosts\": [ \"172.20.0.112\", \"172.20.0.113\", \"172.20.0.114\", \"172.20.0.115\", \"127.0.0.1\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 将 hosts 前面几个 172 开头的IP换成master机器的内网IP 生成 master 证书和私钥 $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes $ ls kubernetes* kubernetes.csr kubernetes-csr.json kubernetes-key.pem kubernetes.pem 创建 admin 证书 管理员使用 kubectl 操作集群，使用下面的 admin 证书具有最高权限，创建 admin-csr.json 文件，内容如下： { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } 生成 admin 证书和私钥 $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin $ ls admin* admin.csr admin-csr.json admin-key.pem admin.pem 创建 kube-proxy 证书 创建 kube-proxy-csr.json 文件，内容如下： { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 生成 kube-proxy 证书和私钥 $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy $ ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem 校验证书 以 master 证书为例： cfssl-certinfo -cert kubernetes.pem 分发证书 将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器的 /etc/kubernetes/ssl 目录下备用: mkdir -p /etc/kubernetes/ssl cp *.pem /etc/kubernetes/ssl Copyright © imroc.io 2018 all right reserved，powered by GitbookUpdated: 2018-03-09 15:07:27 "},"appendix/sync-images-to-docker-hub-using-katacoda.html":{"url":"appendix/sync-images-to-docker-hub-using-katacoda.html","title":"利用Katacoda免费同步Docker镜像到Docker Hub","keywords":"","body":"利用Katacoda免费同步Docker镜像到Docker Hub 无需买服务器，脚本批量同步 为什么要同步 安装kubernetes的时候，我们需要用到 gcr.io/google_containers 下面的一些镜像，在国内是不能直接下载的。如果用 Self Host 方式安装，master 上的组件除开kubelet之外都用容器运行，甚至 CNI 插件也是容器运行，比如 flannel，在 quay.io/coreos 下面，在国内下载非常慢。但是我们可以把这些镜像同步到我们的docker hub仓库里，再配个docker hub加速器，这样下载镜像就很快了。 原理 Katacoda 是一个在线学习平台，在web上提供学习需要的服务器终端，里面包含学习所需的环境，我们可以利用docker的课程的终端来同步，因为里面有docker环境，可以执行 docker login，docker pull，docker tag，docker push 等命令来实现同步镜像。 但是手工去执行命令很麻烦，如果要同步的镜像和tag比较多，手工操作那就是浪费生命，我们可以利用程序代替手工操作，不过 Katacoda 为了安全起见，不允许执行外来的二进制程序，但是可以shell脚本，我写好了脚本，大家只需要粘贴进去根据自己需要稍稍修改下，然后运行就可以了。 Let's Do It 点击 这里 进入docker课程 点击 START SCENARIO 或 终端右上角全屏按钮将终端放大 安装脚本依赖的 jq 命令 apt install jq 登录docker hub docker login 创建脚本并赋予执行权限 touch sync chmod +x sync 编辑脚本，可以使用自带的vim编辑器 vim sync 将脚本粘贴进去 #! /bin/bash docker_repo=\"k8smirror\" # your docker hub username or organization name registry=\"gcr.io\" # the registry of original image, e.g. gcr.io, quay.io repo=\"google_containers\" # the repository name of original image sync_one(){ docker pull ${registry}/${repo}/${1}:${2} docker tag ${registry}/${repo}/${1}:${2} docker.io/${docker_repo}/${1}:${2} docker push docker.io/${docker_repo}/${1}:${2} docker rmi -f ${registry}/${repo}/${1}:${2} docker.io/${docker_repo}/${1}:${2} } sync_all_tags() { for image in $*; do tags_str=`curl https://${registry}/v2/${repo}/$image/tags/list | jq '.tags' -c | sed 's/\\[/\\(/g' | sed 's/\\]/\\)/g' | sed 's/,/ /g'` echo \"$image $tags_str\" src=\" sync_one(){ docker pull ${registry}/${repo}/\\${1}:\\${2} docker tag ${registry}/${repo}/\\${1}:\\${2} docker.io/${docker_repo}/\\${1}:\\${2} docker push docker.io/${docker_repo}/\\${1}:\\${2} docker rmi -f ${registry}/${repo}/\\${1}:\\${2} docker.io/${docker_repo}/\\${1}:\\${2} } tags=${tags_str} echo \\\"$image ${tags_str}\\\" for tag in \\${tags[@]} do sync_one $image \\${tag} done;\" bash -c \"$src\" done } sync_with_tags(){ image=$1 skip=1 for tag in $*; do if [ $skip -eq 1 ]; then skip=0 else sync_one $image $tag fi done } sync_after_tag(){ image=$1 start_tag=$2 tags_str=`curl https://${registry}/v2/${repo}/$image/tags/list | jq '.tags' -c | sed 's/\\[/\\(/g' | sed 's/\\]/\\)/g' | sed 's/,/ /g'` echo \"$image $tags_str\" src=\" sync_one(){ docker pull ${registry}/${repo}/\\${1}:\\${2} docker tag ${registry}/${repo}/\\${1}:\\${2} docker.io/${docker_repo}/\\${1}:\\${2} docker push docker.io/${docker_repo}/\\${1}:\\${2} docker rmi -f ${registry}/${repo}/\\${1}:\\${2} docker.io/${docker_repo}/\\${1}:\\${2} } tags=${tags_str} start=0 for tag in \\${tags[@]}; do if [ \\$start -eq 1 ]; then sync_one $image \\$tag elif [ \\$tag == '$start_tag' ]; then start=1 fi done\" bash -c \"$src\" } get_tags(){ image=$1 curl https://${registry}/v2/${repo}/$image/tags/list | jq '.tags' -c } #sync_with_tags etcd 2.0.12 2.0.13 # sync etcd:2.0.12 and etcd:2.0.13 #sync_after_tag etcd 2.0.8 # sync tag after etcd:2.0.8 #sync_all_tags etcd hyperkube # sync all tags of etcd and hyperkube 脚本中有一些参数需要根据你自己情况修改，可以使用它自带的vim在线修改，也可以在你本地改好在粘贴上去 docker_repo 改为你的Docker Hub账号组织名 registry 改为被同步镜像所在仓库的域名 repo 改为被同步镜像所在仓库的账号或组织名 在脚本最后，可以调用写好的函数来实现镜像同步，举例： 同步一个镜像中指定的一个或多个tag sync_with_tags etcd 2.0.12 2.0.13 从某个tag后面的tag开始一直同步到最后（tag顺序按照字母数字来的，不是上传日期；Katacoda 终端用久了会断连，可能处于安全原因考虑，断开之后可以看tag同步到哪一个了，然后执行类似下面的命令从断连的tag开始同步） sync_after_tag etcd 2.0.8 同步一个或多个镜像的所有tag sync_all_tags etcd hyperkube 最后执行脚本 ./sync 这就开始同步了，Katacoda 服务器在国外，下载 gcr.io 或 quay.io 上那些镜像都很快，上传 Docker Hub 也很快，如果断连了，可以在 Docker Hub 上查最新上传的 tag 是哪个（如：https://hub.docker.com/r/k8smirror/hyperkube/tags/ 把k8smirror改为你的docker用户名或组织名，hyperkube改为镜像名），然后改脚本，用 sync_after_tag 这个函数继续上传。 Copyright © imroc.io 2018 all right reserved，powered by GitbookUpdated: 2018-03-09 15:10:17 "}}